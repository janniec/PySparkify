{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "In this notebook, we will (1) find the best classifier, (2) find the best parameters for the classifier, and (3) find the best set of features to use in our model.   \n",
    "We will evaluate 3 models with our validation set:  \n",
    "1. our chosen classifier with default parameters on all our features  \n",
    "2. our chosen classifier with the best parameters on all our features  \n",
    "3. our chosen classifier with the best paremeters on only the most important features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from time import time\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Process Data\n",
    "Labels and features were generated in the `Sparkify_Exploratory_Analysis_Feature_Engineering.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 85)\n"
     ]
    }
   ],
   "source": [
    "# loading transformed data as spark dataframe\n",
    "data = spark.read.csv('sparkify_data', inferSchema =True, header = True).withColumnRenamed('user_churned', 'label')\n",
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_scale_features(data, features_list=data.columns[2:]):\n",
    "    '''\n",
    "    data [spark dataframe] = data of labels and features with numerical values\n",
    "    features_list [list of strings] = list of names of feature columns\n",
    "    Function to assemble features into a vector, scale it and return it with the labels\n",
    "    input_data [spark dataframe] = dataframe of labels and scaled vectors\n",
    "    '''\n",
    "    # vectorize the features\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='vector')\n",
    "    data = assembler.transform(data)\n",
    "    # scale the feature vectors\n",
    "    scaler = StandardScaler(inputCol='vector', outputCol='features', withMean=True, withStd=True)\n",
    "    scaler_fit = scaler.fit(data)\n",
    "    data = scaler_fit.transform(data)\n",
    "    # select labels and features\n",
    "    input_data = data.select('label', 'features')\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_process_data(data, features_list=data.columns[2:]):\n",
    "    '''\n",
    "    data [spark dataframe] = data of labels and features with numerical values\n",
    "    features_list [list of strings] = list of names of feature columns\n",
    "    Function to split the data into train, test, and validation sets and process them into vectors\n",
    "    processed_train [spark dataframe] = dataframe of labels and feature vectors\n",
    "    processed_test [spark dataframe] = dataframe of labels and feature vectors\n",
    "    processed_validation [spark dataframe] = dataframe of labels and feature vectors\n",
    "    '''\n",
    "    # split the data into sets\n",
    "    train, rest = data.randomSplit([0.6, 0.4], seed=42)\n",
    "    print('train data: ', (train.count(), len(train.columns)))\n",
    "    test, validation = rest.randomSplit([0.5, 0.5], seed=42)\n",
    "    print('test data: ', (test.count(), len(test.columns)))\n",
    "    print('validation data: ', (validation.count(), len(validation.columns)))\n",
    "    # vectorize the features\n",
    "    processed_train = vectorize_scale_features(train, features_list)\n",
    "    processed_test = vectorize_scale_features(test, features_list)\n",
    "    processed_validation = vectorize_scale_features(validation, features_list)\n",
    "    return processed_train, processed_test, processed_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (117, 85)\n",
      "test data:  (41, 85)\n",
      "validation data:  (67, 85)\n"
     ]
    }
   ],
   "source": [
    "train, test, validation = split_process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Classifier Should We Use\n",
    "In order to predict on the full 12GB dataset in the Spark cluster on AWS, we will need the fastest, most accurate model. In addition, we will need the ability to review feature coefficients to pare down the number of features. The data transformation required to generate the 83 features took several hours when run locally. Because the longer we use the Spark cluster in AWS the more expensive it will be, I would like to perform the fewest number of data transformations on the full dataset without sacrificing too much accuracy. For these reasons, we will look at 4 possible classifiers  \n",
    "- Logistic Regression  \n",
    "- Random Forest Classifier  \n",
    "- Gradient Boosted Trees  \n",
    "- Support Vector Machines    \n",
    "  \n",
    "For each classifer, we will use default parameters, train the classifiers on the same train dataset, test the classifier on the same test dataset. We will be looking at each models f1 score as well as speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_predict(model, train_data, test_data, silent=False):\n",
    "    '''\n",
    "    model [pyspark classifier] = intialized classifier\n",
    "    train_data [spark dataframe] = dataframe of labels and feature vectors\n",
    "    test_data [spark dataframe] = dataframe of labels and feature vectors\n",
    "    silent [boolean] = if false, print how long it took train the model and evaluation scores. If false, print nothing\n",
    "    Function to train a classifier, predict with the model on the test set\n",
    "    fitted_model [pyspark classifier model] = a classifier fitted to the train data\n",
    "    predicted_results [pyspark dataframe] = the output of the model with predictions of the test set\n",
    "    '''\n",
    "    start = time()\n",
    "    fitted_model = model.fit(train_data)\n",
    "    predicted_results = fitted_model.transform(test_data)\n",
    "    end = time()\n",
    "    if silent:\n",
    "        pass\n",
    "    else:\n",
    "        print('Training the model took {} seconds.'.format((end-start)))\n",
    "    return fitted_model, predicted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether a user has churned or stayed is an imbalanced class--there are far fewer users who churned. As such, we will be evaluating our models with f1 scores, which is the harmonic mean of precision, which evaluates true positives against false positives, and recall, which evaluates true positives against false negatives. This means that f1 score will be a more class balanced evaluation measure. However, given that Sparkify is considering providing incentives to users who are at risk of churn, there maybe costs associated with false positives-- mistaking content users for those who are at risk and given them unnecessary incentives to stay. So while we want to focus on f1 scores, we don't want to overlook accuracy completely.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set evaluators\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "accu_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "def evaluate_predictions(predicted_results, silent=False):\n",
    "    '''\n",
    "    predicted_results [pyspark dataframe] = the output of the model with predictions of the test set\n",
    "    Function to evaluate the predictions\n",
    "    f1_score [float] = f1 score from evaluation of the model's predictions on the test set\n",
    "    accu_score [float] = accuracy of the model's predictions on the test set\n",
    "    '''\n",
    "    accu_score = accu_evaluator.evaluate(predicted_results.select('label', 'prediction'))\n",
    "    f1_score = f1_evaluator.evaluate(predicted_results.select('label', 'prediction'))\n",
    "    if silent:\n",
    "        pass\n",
    "    else:\n",
    "        print('Accuracy: {0}\\nF1 Score: {1}'.format(accu_score, f1_score))\n",
    "    return f1_score, accu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_evaluate_model(model, train_data, test_data, silent=False):\n",
    "    '''\n",
    "    model [pyspark classifier] = intialized classifier\n",
    "    train_data [spark dataframe] = dataframe of labels and feature vectors\n",
    "    test_data [spark dataframe] = dataframe of labels and feature vectors\n",
    "    silent [boolean] = if false, print how long it took train the model and evaluation scores. If false, print nothing\n",
    "    Function to train a classifier, predict with the model on the test set, and evaluate the predictions\n",
    "    fitted_model [pyspark classifier model] = a classifier fitted to the train data\n",
    "    predicted_results [pyspark dataframe] = the output of the model with predictions of the test set\n",
    "    '''\n",
    "    fitted_model, predicted_results = fit_model_predict(model, train_data, test_data, silent)\n",
    "        \n",
    "    f1_score, accu_score = evaluate_predictions(predicted_results, silent\n",
    "                                               )\n",
    "    return fitted_model, predicted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 6.314727783203125 seconds.\n",
      "Accuracy: 0.6585365853658537\n",
      "F1 Score: 0.6354051927616049\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_fitted_model, lr_predictions = fit_predict_evaluate_model(lr_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_018d014e26c6, numClasses=2, numFeatures=83"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 1.8004083633422852 seconds.\n",
      "Accuracy: 0.6829268292682927\n",
      "F1 Score: 0.574054436196536\n"
     ]
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42)\n",
    "rfc_fitted_model, rfc_predictions = fit_predict_evaluate_model(rfc_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel: uid=RandomForestClassifier_53de0dfc056c, numTrees=20, numClasses=2, numFeatures=83"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 19.85404109954834 seconds.\n",
      "Accuracy: 0.5609756097560976\n",
      "F1 Score: 0.5609756097560975\n"
     ]
    }
   ],
   "source": [
    "gbt_model = GBTClassifier(featuresCol='features', labelCol='label', seed=42)\n",
    "gbt_fitted_model, gbt_predictions = fit_predict_evaluate_model(gbt_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTClassificationModel: uid = GBTClassifier_136202859156, numTrees=20, numClasses=2, numFeatures=83"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 26.991944074630737 seconds.\n",
      "Accuracy: 0.6097560975609756\n",
      "F1 Score: 0.5833202202989772\n"
     ]
    }
   ],
   "source": [
    "svm_model = LinearSVC(featuresCol='features', labelCol='label')\n",
    "svm_fitted_model, svm_predictions = fit_predict_evaluate_model(svm_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVCModel: uid=LinearSVC_a3964e0705ae, numClasses=2, numFeatures=83"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected all the classifiers didn't do particularly well when using default parameters. While logistic regression had a slightly better f1 score, the random forest classifier had the better accuracy. Significantly, the random forest classifer was by far the fastest classifer. Given that I anticipate performance to improve with any of the classifier, and we are building a model that may need to run regularly on large data sets, I will proceed with the fastest classifier - random forest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Validation: Validate the Best Default Classifier\n",
    "Let's set a baseline evaluation to beat. We will see how the default random forest classifier performs on our validation data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 2.3147828578948975 seconds.\n",
      "Accuracy: 0.7910447761194029\n",
      "F1 Score: 0.7119402985074627\n"
     ]
    }
   ],
   "source": [
    "# rfc_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42)\n",
    "rfc1_fitted_model, rfc1_predictions = fit_predict_evaluate_model(rfc_model, train, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, our default random forest classifier performed pretty well on our validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Parameters Should We Use\n",
    "[Documentation for Pyspark's Random Forest Classifier](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html) shows the following default parameters:\n",
    "- maxBins=32\n",
    "- maxDepth=5\n",
    "- numTrees=20\n",
    "- impurity='gini'\n",
    "  \n",
    "To find better parameters, we will explore `maxDepth`, `maxBins`, and `numTrees` values both lower than and higher than the defaults in the Parameter Grid. We will also consider `entropy` as an alternative `impurity`, the metric used to calculate information gain at each node.  \n",
    "  \n",
    "In addition to Parameter Grid, we will also use Cross Validation. The random forest algorithm has 2 random components when training models: (1) each tree trains on a random sample of the data and (2) each tree trains on a random subset of the features. A single experiment could result in a lucky jump in the f1 score. Instead, we will use a cross validator to split the train dataset into 3 folds, train a model on 2 folds and test on the third, rotate the folds and repeat twice more. Ultimately, for each combination of parameters, 3 models will be trained and evaluated, and we will get an average score. This will ensure that we get a combination of parameters that actually improved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up the Cross Validator\n",
    "## Evaluator\n",
    "# f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "## classifier\n",
    "# rfc_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42)\n",
    "\n",
    "## Parameter Grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rfc_model.maxBins, [10, 30, 50]) \\\n",
    "    .addGrid(rfc_model.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(rfc_model.numTrees, [5, 25, 50]) \\\n",
    "    .addGrid(rfc_model.impurity,['entropy', 'gini']) \\\n",
    "    .build()\n",
    "\n",
    "## Cross Validator\n",
    "cv = CrossValidator(estimator=rfc_model,\n",
    "                      evaluator=f1_evaluator, \n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the Cross Validator\n",
    "cv_model_fitted, cv_predictions = fit_predict_evaluate_model(cv, train, test, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxBins</th>\n",
       "      <th>maxDepth</th>\n",
       "      <th>numTrees</th>\n",
       "      <th>impurity</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.761377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.717993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.715141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.705570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.700224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.699738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.699532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.697388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.696782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.691625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.691578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.690871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.687710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.687240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.687022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.685841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.683998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.681802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.681437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.681007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.679901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.674712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.671176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.669846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.668178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.667079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.667079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.666771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.666771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.666590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.666399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.663763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.662674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.662411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.662054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.659705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.658190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.657956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.656633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.654021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.652310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.649801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.642524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    maxBins  maxDepth  numTrees impurity  f1_score\n",
       "0        30        10         5     gini  0.761377\n",
       "1        10         5        25     gini  0.717993\n",
       "2        30        10        50  entropy  0.715141\n",
       "3        10         2         5     gini  0.705570\n",
       "4        30        10        25  entropy  0.700224\n",
       "5        10        10        50  entropy  0.699738\n",
       "6        10         2         5  entropy  0.699532\n",
       "7        50        10         5     gini  0.697388\n",
       "8        50         5        25     gini  0.696782\n",
       "9        50        10        25  entropy  0.691625\n",
       "10       50         5         5     gini  0.691578\n",
       "11       10         5        25  entropy  0.690871\n",
       "12       50         5        50     gini  0.687710\n",
       "13       10        10        25     gini  0.687240\n",
       "14       50        10        25     gini  0.687022\n",
       "15       30         5        25  entropy  0.685841\n",
       "16       30         5        50  entropy  0.683998\n",
       "17       30         5        50     gini  0.681802\n",
       "18       50         5        25  entropy  0.681437\n",
       "19       10        10        25  entropy  0.681007\n",
       "20       10         5        50     gini  0.679901\n",
       "21       50        10        50     gini  0.674712\n",
       "22       10         2        25  entropy  0.671176\n",
       "23       10         2        25     gini  0.671176\n",
       "24       30         2        25  entropy  0.671176\n",
       "25       30         2        25     gini  0.671176\n",
       "26       50         2        25  entropy  0.671176\n",
       "27       50         2        25     gini  0.671176\n",
       "28       10         2        50  entropy  0.671176\n",
       "29       10         2        50     gini  0.671176\n",
       "30       30         2        50  entropy  0.671176\n",
       "31       30         2        50     gini  0.671176\n",
       "32       50         2        50  entropy  0.671176\n",
       "33       50         2        50     gini  0.671176\n",
       "34       30         5         5     gini  0.669846\n",
       "35       50         5        50  entropy  0.668178\n",
       "36       30         2         5     gini  0.667079\n",
       "37       50         2         5     gini  0.667079\n",
       "38       30         2         5  entropy  0.666771\n",
       "39       50         2         5  entropy  0.666771\n",
       "40       30        10        25     gini  0.666590\n",
       "41       10         5         5     gini  0.666399\n",
       "42       10         5         5  entropy  0.663763\n",
       "43       10         5        50  entropy  0.662674\n",
       "44       50        10        50  entropy  0.662411\n",
       "45       50        10         5  entropy  0.662054\n",
       "46       10        10         5     gini  0.659705\n",
       "47       30        10        50     gini  0.658190\n",
       "48       30         5        25     gini  0.657956\n",
       "49       10        10        50     gini  0.656633\n",
       "50       10        10         5  entropy  0.654021\n",
       "51       30         5         5  entropy  0.652310\n",
       "52       30        10         5  entropy  0.649801\n",
       "53       50         5         5  entropy  0.642524"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The scores for each set of parameters\n",
    "parameters_combo = [{p.name: v for p, v in m.items()} for m in cv_model_fitted.getEstimatorParamMaps()]\n",
    "cv_scores = pd.DataFrame(parameters_combo)\n",
    "cv_scores['f1_score'] = cv_model_fitted.avgMetrics\n",
    "cv_scores = cv_scores.sort_values(by=['f1_score', 'maxDepth', 'numTrees', 'maxBins'], ascending=[False, True, True, True]).reset_index(drop=True)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are gini with 30 maxBins, 10 maxDepth, and 5 trees\n"
     ]
    }
   ],
   "source": [
    "### best parameters\n",
    "max_bins = cv_scores.iloc[0]['maxBins']\n",
    "max_depth = cv_scores.iloc[0]['maxDepth']\n",
    "num_trees = cv_scores.iloc[0]['numTrees']\n",
    "impurity = cv_scores.iloc[0]['impurity']\n",
    "print('The best parameters are {0} with {1} maxBins, {2} maxDepth, and {3} trees'.format(impurity, max_bins, max_depth, num_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for our Random Forest Classifier are as follows:\n",
    "- `maxBins`: 30 (the default was 32)  \n",
    "MaxBins is a parameter found in the Pyspark's Random Forest Classfier, but not in SciKit Learn's counterpart. PySpark's documentation claims that the number of bins are used to discretize continuous features. Increasing this value allows to algorithm to make more fine grained splits. I believe that increasing this number too high could lead to overfitting on the training data, while lower values could lead to under fitting. So in the parameter grid, I considered a significantly lower number of bins than the default, 10, and a slightly lower number of bins than the default, 30. The top performing models appear to gravitated towards a lower number of bins. \n",
    "- `maxDepth`: 10 (the default was 5)  \n",
    "This parameter determines the size of the individual trees. A larger number here, could lead to over fitting. However, with multiple trees in the algorithm, this risk is lower.  \n",
    "- `numTrees`: 5 (the default was 20)  \n",
    "This parameter determines the number of trees. A lower number here, could lead to over fitting. The combination of a higher maxDepth along with a lower numTrees indicates that the default model was too underfitting. \n",
    "- `impurity`: `gini` (the default was 'gini')  \n",
    "This parameter determines how trees split data a branch. In short, `gini` measures the probablity of a random sample of data points being classified incorrectly at a branch/node. `entropy` measures the impurity of information (the labels being one class is considered pure) at a branch. All other parameters being equal, switching this parameter to `entropy` would have dropped the f1 score by about 10%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Validation: Validate Classifier with the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took 2.5150463581085205 seconds.\n",
      "Accuracy: 0.7910447761194029\n",
      "F1 Score: 0.7652003142183817\n"
     ]
    }
   ],
   "source": [
    "# How does the best set of parameters do without cross validation\n",
    "rfc2_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42, \\\n",
    "                                    impurity = impurity, maxDepth=max_depth, numTrees=num_trees, maxBins=max_bins)\n",
    "rfc2_fitted_model, rfc2_predictions = fit_predict_evaluate_model(rfc2_model, train, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first validation: \n",
    "- Training the model took 2.3147828578948975 seconds.\n",
    "- Accuracy: 0.7910447761194029\n",
    "- F1 Score: 0.7119402985074627  \n",
    "  \n",
    "With updated parameters but on the same train and validation datasets, our F1 score improved by 5%. Coincidentially, our accuracy is exactly the same. This indicates that our second model is better a predicting churned users but is offsetting the accuracy by misidentifying users who would have stayed as at risk of churn. In addition, the time to train the second model barely increased by .2 seconds. We could stop here. However, for the sake of cost, time, and computational resources, we should consider feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Features Should We Use?\n",
    "When using the Spark cluster on AWS, the longer we need to run the code, the more it will cost us. The data transformation, required to create the features, was the most time consuming portion of this project. It took a few hours when run locally. So Sparkify may want to limit the number of features. As such, we will look at the top important features and rerun our best classifier with the best parameters on fewer features.  Ideally, we will find a smaller set of features to run in our model without sacrificing too much of the f1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>page_count_Thumbs_Down</td>\n",
       "      <td>0.075904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>page_count_Logout</td>\n",
       "      <td>0.063622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>status_count_200</td>\n",
       "      <td>0.050785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>method_count_PUT</td>\n",
       "      <td>0.045865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_song_length</td>\n",
       "      <td>0.041142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>page_count_NextSong</td>\n",
       "      <td>0.040795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total_session_length</td>\n",
       "      <td>0.038328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>level_count_free_days</td>\n",
       "      <td>0.036859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>avg_session_length</td>\n",
       "      <td>0.036561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>page_count_Settings</td>\n",
       "      <td>0.034629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>state_MS</td>\n",
       "      <td>0.034548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>page_count_Add_Friend</td>\n",
       "      <td>0.034146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>page_count_Error</td>\n",
       "      <td>0.033961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>session_count</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>level_count_paid_days</td>\n",
       "      <td>0.031732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>log_count</td>\n",
       "      <td>0.029312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>state_VA</td>\n",
       "      <td>0.028503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>method_count_GET</td>\n",
       "      <td>0.028051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>page_count_Roll_Advert</td>\n",
       "      <td>0.023530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>page_count_Add_to_Playlist</td>\n",
       "      <td>0.022925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>page_count_Upgrade</td>\n",
       "      <td>0.022540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>page_count_Downgrade</td>\n",
       "      <td>0.018286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>song_count</td>\n",
       "      <td>0.017751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>state_NY</td>\n",
       "      <td>0.017503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>page_count_Help</td>\n",
       "      <td>0.017072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>state_NJ</td>\n",
       "      <td>0.015568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>os_Linux</td>\n",
       "      <td>0.014869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>level_count_free_logs</td>\n",
       "      <td>0.013511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>status_count_307</td>\n",
       "      <td>0.013074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>total_song_length</td>\n",
       "      <td>0.012705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>page_count_Thumbs_Up</td>\n",
       "      <td>0.011793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>state_OR</td>\n",
       "      <td>0.009148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>state_DC</td>\n",
       "      <td>0.009143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>artist_count</td>\n",
       "      <td>0.008509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>state_TX</td>\n",
       "      <td>0.007923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>state_MO</td>\n",
       "      <td>0.007275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>page_count_Save_Settings</td>\n",
       "      <td>0.007240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>state_WA</td>\n",
       "      <td>0.005242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lastlevel_paid</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gender_male</td>\n",
       "      <td>0.002915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>level_count_paid_logs</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  Importance\n",
       "0       page_count_Thumbs_Down    0.075904\n",
       "1            page_count_Logout    0.063622\n",
       "2             status_count_200    0.050785\n",
       "3             method_count_PUT    0.045865\n",
       "4              avg_song_length    0.041142\n",
       "5          page_count_NextSong    0.040795\n",
       "6         total_session_length    0.038328\n",
       "7        level_count_free_days    0.036859\n",
       "8           avg_session_length    0.036561\n",
       "9          page_count_Settings    0.034629\n",
       "10                    state_MS    0.034548\n",
       "11       page_count_Add_Friend    0.034146\n",
       "12            page_count_Error    0.033961\n",
       "13               session_count    0.031915\n",
       "14       level_count_paid_days    0.031732\n",
       "15                   log_count    0.029312\n",
       "16                    state_VA    0.028503\n",
       "17            method_count_GET    0.028051\n",
       "18      page_count_Roll_Advert    0.023530\n",
       "19  page_count_Add_to_Playlist    0.022925\n",
       "20          page_count_Upgrade    0.022540\n",
       "21        page_count_Downgrade    0.018286\n",
       "22                  song_count    0.017751\n",
       "23                    state_NY    0.017503\n",
       "24             page_count_Help    0.017072\n",
       "25                    state_NJ    0.015568\n",
       "26                    os_Linux    0.014869\n",
       "27       level_count_free_logs    0.013511\n",
       "28            status_count_307    0.013074\n",
       "29           total_song_length    0.012705\n",
       "30        page_count_Thumbs_Up    0.011793\n",
       "31                    state_OR    0.009148\n",
       "32                    state_DC    0.009143\n",
       "33                artist_count    0.008509\n",
       "34                    state_TX    0.007923\n",
       "35                    state_MO    0.007275\n",
       "36    page_count_Save_Settings    0.007240\n",
       "37                    state_WA    0.005242\n",
       "38              lastlevel_paid    0.003968\n",
       "39                 gender_male    0.002915\n",
       "40       level_count_paid_logs    0.000855"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coefficients = rfc2_fitted_model.featureImportances\n",
    "feature_names = data.columns[2:]\n",
    "feature_importance = pd.DataFrame(list(zip(feature_names, feature_coefficients)),\\\n",
    "                                  columns=['Feature', 'Importance'])\\\n",
    "                                .sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "feature_importance[feature_importance['Importance']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41 out of the 85 features were used to train the second model. Just looking at the top few, high values in `Thumb Down` and `Logout` would strongly indicate disatisfaction with the service. Conversely, `200`, HTTP status code that the service is working, `PUT` HTTP method request to store data, `avg_song_lenght` and `NextSong` indicate engagement with the service. If those values were high, they would signal a user's satisfaction with the service. Whereas, low numbers would signal a user's disatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cv_features(num_features):   \n",
    "    '''\n",
    "    num_features [int] = number of most important features\n",
    "    Function to trim the data set down to a select number of the most important features and split it, \n",
    "    run the new sets through a pipeline to process, train a model, predict on the test set, and evaluate the predictions\n",
    "    features_list [list of strings] = list of names of feature columns\n",
    "    f1_score [float] = f1 score from evaluation of the model's predictions on the test set\n",
    "    accu_score [float] = accuracy of the model's predictions on the test set\n",
    "    '''\n",
    "    # Set up data with features\n",
    "    features_list= list(feature_importance['Feature'])[:num_features]\n",
    "    features_data = data.select( ['userId', 'label'] + features_list)\n",
    "    train, rest = features_data.randomSplit([0.6, 0.4], seed=42)\n",
    "    test, validation = rest.randomSplit([0.5, 0.5], seed=42)\n",
    "    \n",
    "    # Set up pipelines\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='vector')\n",
    "    scaler = StandardScaler(inputCol='vector', outputCol='features', withMean=True, withStd=True)\n",
    "#     rfc2_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42, \\\n",
    "#                                     impurity = impurity, maxDepth=max_depth, numTrees=num_trees, maxBins=max_bins)\n",
    "\n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "    \n",
    "    cv = CrossValidator(estimator=rfc2_model,\n",
    "                      evaluator=f1_evaluator, \n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      numFolds=3)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler, cv])\n",
    "    \n",
    "    pipeline_fitted, pipeline_predictions = fit_model_predict(pipeline, train, test, silent=True)\n",
    "    f1_score, accu_score = evaluate_predictions(pipeline_predictions, silent=True)\n",
    "    return features_list, f1_score, accu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with various feature selection means that for each set of features, we will need to split, vectorize and scale our data sets again before we train our models. In order to speed this up, we will use PySpark's pipeline preprocessing and cross validation for a more reliable evaluation of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_top_features</th>\n",
       "      <th>feature_list</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.72688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.707052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.690917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.672256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.672256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.653789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.651885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.651885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.651885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.639585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>['page_count_Thumbs_Down']</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.635405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.63472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.63472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.63472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.63472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.624308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.617632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.617632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.617632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.617632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.609101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.609101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.609101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.593865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.593865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.58332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.58332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>27</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.578523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout']</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.563008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.563008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>18</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.563008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.535846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>12</td>\n",
       "      <td>['page_count_Thumbs_Down', 'page_count_Logout'...</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.508384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_top_features                                       feature_list  \\\n",
       "0                 4  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "1                37  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "2                38  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "3                16  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "4                30  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "5                17  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "6                14  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "7                15  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "8                19  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "9                22  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "10                1                         ['page_count_Thumbs_Down']   \n",
       "11                3  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "12               13  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "13               25  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "14               40  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "15                9  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "16                7  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "17               28  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "18               29  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "19               36  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "20               11  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "21               23  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "22               39  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "23                5  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "24               24  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "25               26  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "26               31  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "27                8  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "28               20  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "29               21  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "30               27  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "31               32  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "32               33  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "33               34  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "34               35  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "35                2    ['page_count_Thumbs_Down', 'page_count_Logout']   \n",
       "36               10  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "37               18  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "38                6  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "39               12  ['page_count_Thumbs_Down', 'page_count_Logout'...   \n",
       "\n",
       "    accuracy  f1_score  \n",
       "0   0.756098   0.72688  \n",
       "1   0.731707  0.707052  \n",
       "2   0.731707  0.690917  \n",
       "3   0.707317  0.672256  \n",
       "4   0.707317  0.672256  \n",
       "5   0.682927  0.653789  \n",
       "6   0.707317  0.651885  \n",
       "7   0.707317  0.651885  \n",
       "8   0.707317  0.651885  \n",
       "9   0.731707  0.639585  \n",
       "10  0.658537  0.635405  \n",
       "11  0.682927   0.63472  \n",
       "12  0.682927   0.63472  \n",
       "13  0.682927   0.63472  \n",
       "14  0.682927   0.63472  \n",
       "15  0.707317  0.624308  \n",
       "16  0.658537  0.617632  \n",
       "17  0.658537  0.617632  \n",
       "18  0.658537  0.617632  \n",
       "19  0.658537  0.617632  \n",
       "20  0.682927  0.609101  \n",
       "21  0.682927  0.609101  \n",
       "22  0.682927  0.609101  \n",
       "23  0.658537  0.593865  \n",
       "24  0.658537  0.593865  \n",
       "25  0.609756   0.58332  \n",
       "26  0.609756   0.58332  \n",
       "27  0.634146  0.578523  \n",
       "28  0.634146  0.578523  \n",
       "29  0.634146  0.578523  \n",
       "30  0.634146  0.578523  \n",
       "31  0.634146  0.578523  \n",
       "32  0.634146  0.578523  \n",
       "33  0.634146  0.578523  \n",
       "34  0.634146  0.578523  \n",
       "35  0.609756  0.563008  \n",
       "36  0.609756  0.563008  \n",
       "37  0.609756  0.563008  \n",
       "38  0.609756  0.535846  \n",
       "39  0.560976  0.508384  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score the sets of top features\n",
    "features_scores = pd.DataFrame(columns=['num_top_features', 'feature_list', 'accuracy', 'f1_score'])\n",
    "\n",
    "for index, num_features in enumerate(range(1, len(feature_importance[feature_importance['Importance']>0]))):\n",
    "    features_list, f1_score, accu_score = score_cv_features(num_features)\n",
    "    features_scores.loc[index, 'num_top_features'] = num_features\n",
    "    features_scores.loc[index, 'feature_list'] = str(features_list)\n",
    "    features_scores.loc[index, 'accuracy'] = accu_score\n",
    "    features_scores.loc[index, 'f1_score'] = f1_score\n",
    "features_scores = features_scores.sort_values(by=['f1_score', 'num_top_features'], ascending=[False, True]).reset_index(drop=True)\n",
    "features_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation on the various sets of features show that 4 features are really necessary to predict user Churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important features are:  ['page_count_Thumbs_Down', 'page_count_Logout', 'status_count_200', 'method_count_PUT']\n"
     ]
    }
   ],
   "source": [
    "# Best set of Top Features\n",
    "top_features_list = list(feature_importance['Feature'])[:int(features_scores.iloc[0]['num_top_features'])]\n",
    "print('The most important features are: ', top_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Validation: Validate Classifier with Best Parameters on only the Most Important Features\n",
    "The best model, the best parameters, only the important features on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (117, 6)\n",
      "test data:  (41, 6)\n",
      "validation data:  (67, 6)\n",
      "Training the model took 2.378504514694214 seconds.\n",
      "Accuracy: 0.7164179104477612\n",
      "F1 Score: 0.7017556167458828\n"
     ]
    }
   ],
   "source": [
    "data2 = data.select(['userId', 'label'] + top_features_list)\n",
    "train2, test2, validation2 = split_process_data(data2, top_features_list)\n",
    "rfc3_model = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42, \\\n",
    "                                    impurity=impurity, maxDepth=max_depth, numTrees=num_trees, maxBins=max_bins)\n",
    "rfc3_fitted_model, rfc3_predictions = fit_predict_evaluate_model(rfc3_model, train2, validation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first validation: \n",
    "- Training the model took 2.3147828578948975 seconds.\n",
    "- Accuracy: 0.7910447761194029\n",
    "- F1 Score: 0.7119402985074627  \n",
    "  \n",
    "In our second validation:\n",
    "- Training the model took 2.5150463581085205 seconds.\n",
    "- Accuracy: 0.7910447761194029\n",
    "- F1 Score: 0.7652003142183817\n",
    "\n",
    "Unsurprisingly, our 3rd model didn't perform as well. The F1 score dropped by 6% from the second model, 1% from the default model. Accuracy dropped about 8% from both models. The time to train third model dropped to about the time it took to train the default model. Despite, this dip in performance, this third model is a model worth consideration. It only requires 4 features, which would probably take less than an hour of data processing when run locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Summary\n",
    "  \n",
    "Sparkify asked us to predict which users are at risk of churn, cancelling the service, so that they can incentivice those users to stay. The raw data was event-level log data with timestamps that had to be converted, `location` and `useragent` data that had to be simplified. The logs had to be aggregated into user-level data. Data exploration revealed that interestingly users who were on the paid tier were more likely to stay. \n",
    "  \n",
    "Because this was a preliminary experiment, I was comprehensive in my feature generation, which took a few hours to create 85 features. It became apparent that such comprehensive feature generation could be exorbitantly time consuming and costly to perform on the full data set. Our analysis was performed on a mere 128MB subset of a 12GB data set. So I mainly considered classifier model that would allow me to rank feature importance. I evaluated the model performances with f1 score because we were predicting an imbalanced class--only 23% of users churned. The default versions of the classifier performed similarly, but I chose to continue with the Random Forest Classifier because it was signficantly faster and we are building a model that may need to run regularly on a large dataset.\n",
    "\n",
    "## Improvement\n",
    "Through experimentation, I produced 3 models to run on our validation data set:  \n",
    "- Random Forest Classifier with default parameters trained on 85 features. F1 Score: 71%  \n",
    "This model became our baseline for performance. \n",
    "- Random Forest Classifier with a better set of parameters trained on 85 features. F1 Score: 76%\n",
    "The `impurity` and `maxBin` parameters mostly remained the same as the default values. But the model performed much better with  a higher `maxDepth`  than the default and much lower `numTrees` than the default, which combined indicated that the default model was underfit and a better model just needed to be closer fit to the training data.\n",
    "- Random Forest Classifier with the same better set of parameters train on 4 features. F1 Score: 70% \n",
    "Because data processing took so long, I continued my experiments to include feature selection. Cross validation on various sets of features indicated that only 4 features were really necessary to predict user churn--`Thumbs Down`, `Log out`, `200` HTTP Status and `PUT` HTTP method request. Although training a model on just 4 features would sacrifice 6% in performance, it would speed up data process signficantly. \n",
    "  \n",
    "Ultimately we improved our model by 5% and we found the most crucial features. I expect this model would improve even further when it is trained on a much larger set of the data. \n",
    "\n",
    "## Reflection\n",
    "Although Sparkify asked us to predict users who were at risk of churning, its ultimate goal is to encourage users to stay. Exploratory Data Analysis indicated that user who churned weren't encountering `Error` pages or `404` status issues.  Feature importance showed us that the top feature influencing the models was `Thumbs Down`, meaning that a high occurence of this event in a user's account indicates dissatisfaction with the songs. While Sparkify said that they were considering providing discounts and incentives for users to stay, might I recommend that Sparkify improves its song recommendation engine. Perhaps the better approach to convincing users to stay is preventing users from streaming songs they would `Thumbs Down`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
